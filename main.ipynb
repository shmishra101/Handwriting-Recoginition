{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import csv\n",
    "import math\n",
    "import matplotlib.pyplot\n",
    "from matplotlib import pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method to create the feature data\n",
    "def createfeaturedata(pairfile,featuresfile,outputfile,featurelength,index):\n",
    "    with open(pairfile,'rU') as f:\n",
    "         pairs=csv.reader(f,delimiter=',')\n",
    "         headers=next(pairs)\n",
    "         data=list(pairs)\n",
    "         data=np.array(data)\n",
    "    with open(featuresfile,'rU') as fi:\n",
    "         pair=csv.reader(fi,delimiter=',')\n",
    "         featureheaders=next(pair)\n",
    "         featuredata=list(pair)\n",
    "         featuredata=np.array(featuredata)\n",
    "    with open(outputfile, 'a',newline='') as out:\n",
    "         writer=csv.writer(out)\n",
    "         for i in range(0,len(data),index):\n",
    "             firstpair=data[i][0]\n",
    "             secondpair=data[i][1]\n",
    "             target=int(data[i][2])\n",
    "             a=np.append(firstpair,secondpair)\n",
    "             a=np.append(a,[target])\n",
    "             for j in range(0,len(featuredata)):\n",
    "                 if(firstpair==featuredata[j][0]):\n",
    "                    firstfeature=featuredata[j][1:featurelength+1]\n",
    "                    break\n",
    "             for j in range(0,len(featuredata)):\n",
    "                 if(secondpair==featuredata[j][0]):\n",
    "                    secondfeature=featuredata[j][1:featurelength+1]\n",
    "                    break\n",
    "             if(featurelength==18 or featurelength==1024):\n",
    "                a=np.append(a,[firstfeature.astype(int),secondfeature.astype(int)])\n",
    "             else:\n",
    "                a=np.append(a,np.absolute(firstfeature.astype(int)-secondfeature.astype(int))) \n",
    "             writer.writerow(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mapping Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shmis\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:3: DeprecationWarning: 'U' mode is deprecated\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n",
      "C:\\Users\\shmis\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:8: DeprecationWarning: 'U' mode is deprecated\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Human Features concatenation dataset generated\n",
      "Human Features subtraction dataset generated\n",
      "Please wait while GSC Features dataset is genetating it will take several minutes\n",
      "GSC features concatenation dataset generated\n",
      "Please wait while GSC Subtraction Features dataset is generating it will take several minutes\n",
      "GSC features subtraction dataset generated\n"
     ]
    }
   ],
   "source": [
    "# Creating Human features concatenation dataset \n",
    "createfeaturedata('same_pairs.csv','HumanObserved-Features-Data.csv','output_conc_human.csv',18,1)\n",
    "createfeaturedata('diffn_pairs.csv','HumanObserved-Features-Data.csv','output_conc_human.csv',18,390)\n",
    "print('Human Features concatenation dataset generated')\n",
    "# Creating Human Features Subtraction  dataset\n",
    "createfeaturedata('same_pairs.csv','HumanObserved-Features-Data.csv','output_subt_human.csv',9,1)\n",
    "createfeaturedata('diffn_pairs.csv','HumanObserved-Features-Data.csv','output_subt_human.csv',9,390)\n",
    "print('Human Features subtraction dataset generated')\n",
    "print('Please wait while GSC Features dataset is genetating it will take several minutes')\n",
    "# Creating GSC Features Concatenation dataset\n",
    "createfeaturedata('same_pairs_gsc.csv','GSC-Features.csv','output_conc_gsc.csv',1024,7)\n",
    "createfeaturedata('diffn_pairs_gsc.csv','GSC-Features.csv','output_conc_gsc.csv',1024,70)\n",
    "print('GSC features concatenation dataset generated')\n",
    "print('Please wait while GSC Subtraction Features dataset is generating it will take several minutes')\n",
    "# Creating GSC subtraction dataset\n",
    "createfeaturedata('same_pairs_gsc.csv','GSC-Features.csv','output_subt_gsc.csv',512,7)\n",
    "createfeaturedata('diffn_pairs_gsc.csv','GSC-Features.csv','output_subt_gsc.csv',512,70)\n",
    "print('GSC features subtraction dataset generated')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method to get the \n",
    "def GetTargetVector(shuffledata):\n",
    "    t = []\n",
    "    for i in range(0,len(shuffledata)):\n",
    "            t.append(shuffledata.iloc[i,2])\n",
    "    #print(\"Raw Training Generated..\")\n",
    "    return np.array(t)\n",
    "# Method to create feature data \n",
    "def GenerateRawData(shuffledata,featurelength):    \n",
    "    dataMatrix = []\n",
    "    for i in range(0,len(shuffledata)):\n",
    "        dataMatrix.append(shuffledata.iloc[i,3:featurelength+3])\n",
    "    dataMatrix=np.transpose(dataMatrix).astype(int) \n",
    "    T_len = int(math.ceil(len(dataMatrix[0])*0.01*80))\n",
    "    trainingdata=dataMatrix[:,0:T_len]\n",
    "    # Deleting the zeros column\n",
    "    zeros=np.where(~trainingdata.any(axis=1))[0]\n",
    "    dataMatrix=np.delete(dataMatrix,zeros,axis=0)\n",
    "    return dataMatrix\n",
    "# Method to create Training Data\n",
    "def GenerateTrainingDataMatrix(rawData, TrainingPercent):\n",
    "    T_len = int(math.ceil(len(rawData[0])*0.01*TrainingPercent))\n",
    "    d2 = rawData[:,0:T_len]\n",
    "    return d2\n",
    "# Method to create Training Target\n",
    "def GenerateTrainingTarget(rawTraining,TrainingPercent):\n",
    "    TrainingLen = int(math.ceil(len(rawTraining)*(TrainingPercent*0.01)))\n",
    "    t = rawTraining[:TrainingLen]\n",
    "    return t\n",
    "\n",
    "# Generating Validation data matrix and Validation target vector from 80 to 90 % of dataset\n",
    "def GenerateValData(rawData, ValPercent, TrainingCount): \n",
    "    valSize = int(math.ceil(len(rawData[0])*ValPercent*0.01))\n",
    "    V_End = TrainingCount + valSize\n",
    "    dataMatrix = rawData[:,TrainingCount+1:V_End] \n",
    "    return dataMatrix\n",
    "def GenerateValTargetVector(rawData, ValPercent, TrainingCount): \n",
    "    valSize = int(math.ceil(len(rawData)*ValPercent*0.01))\n",
    "    V_End = TrainingCount + valSize\n",
    "    t =rawData[TrainingCount+1:V_End]\n",
    "    return t\n",
    "# Generating Big Sigma matrix .This matrix will be same for three datasets\n",
    "def GenerateBigSigma(Data,TrainingPercent):\n",
    "    BigSigma    = np.zeros((len(Data),len(Data)))\n",
    "    DataT       = np.transpose(Data)\n",
    "    TrainingLen = math.ceil(len(DataT)*(TrainingPercent*0.01))        \n",
    "    varVect     = []\n",
    "    for i in range(0,len(DataT[0])):\n",
    "        vct = []\n",
    "        for j in range(0,int(TrainingLen)):\n",
    "            vct.append(Data[i][j])  \n",
    "        # Calculating the variance of feature\n",
    "        varVect.append(np.var(vct))\n",
    "    \n",
    "    for j in range(len(Data)):\n",
    "        BigSigma[j][j] = varVect[j]\n",
    "\n",
    "    return BigSigma\n",
    "# Get Scalar and GetRadialBasisOut function Calulates the following equation:- exp(−(x−µj)(Σ)-1(x−µj)/2)\n",
    "def GetScalar(DataRow,MuRow, BigSigInv):  \n",
    "    R = np.subtract(DataRow,MuRow)\n",
    "    T = np.dot(BigSigInv,np.transpose(R))  \n",
    "    L = np.dot(R,T)\n",
    "    return L\n",
    "def GetRadialBasisOut(DataRow,MuRow, BigSigInv):    \n",
    "    phi_x = math.exp(-0.5*GetScalar(DataRow,MuRow,BigSigInv))\n",
    "    return phi_x\n",
    "\n",
    "# Caluclating the Phi matrix \n",
    "def GetPhiMatrix(Data, MuMatrix, BigSigma, TrainingPercent):\n",
    "    DataT = np.transpose(Data)\n",
    "    TrainingLen = math.ceil(len(DataT)*(TrainingPercent*0.01)) \n",
    "    PHI = np.zeros((int(TrainingLen),len(MuMatrix))) \n",
    "    BigSigInv = np.linalg.inv(BigSigma)\n",
    "    for  C in range(0,len(MuMatrix)):\n",
    "        for R in range(0,int(TrainingLen)):\n",
    "            PHI[R][C] = GetRadialBasisOut(DataT[R], MuMatrix[C], BigSigInv)\n",
    "    return PHI\n",
    "def GetValTest(VAL_PHI,W):\n",
    "    Y = np.dot(W,np.transpose(VAL_PHI))\n",
    "    return Y\n",
    "# Calculating Root mean square which is equivalent to likelihood function\n",
    "def GetErms(VAL_TEST_OUT,ValDataAct):\n",
    "    sum = 0.0\n",
    "    t=0\n",
    "    accuracy = 0.0\n",
    "    counter = 0\n",
    "    val = 0.0\n",
    "    for i in range (0,len(VAL_TEST_OUT)):\n",
    "        sum = sum + math.pow((ValDataAct[i] - VAL_TEST_OUT[i]),2)\n",
    "        if(int(np.around(VAL_TEST_OUT[i], 0)) == ValDataAct[i]):\n",
    "            counter+=1\n",
    "    accuracy = (float((counter*100))/float(len(VAL_TEST_OUT)))\n",
    "    return (str(accuracy) + ',' +  str(math.sqrt(sum/len(VAL_TEST_OUT))))\n",
    "def linearregression(TrainingTarget,Training_Phi,learningRate,iteration):\n",
    "    W_Now        = np.dot(0,np.zeros(M))\n",
    "    La           = 2\n",
    "    for i in range(0,iteration):\n",
    "    \n",
    "         #print ('---------Iteration: ' + str(i) + '--------------')\n",
    "         Delta_E_D     = -np.dot((TrainingTarget[i] - np.dot(np.transpose(W_Now),Training_Phi[i])),Training_Phi[i])\n",
    "         La_Delta_E_W  = np.dot(La,W_Now)\n",
    "         Delta_E       = np.add(Delta_E_D,La_Delta_E_W)    \n",
    "         Delta_W       = -np.dot(learningRate,Delta_E)\n",
    "         W_T_Next      = W_Now + Delta_W\n",
    "         W_Now         = W_T_Next\n",
    "    return W_Now"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading Features from File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading Human Concatenation Data\n",
    "dataframe = pd.read_csv('output_conc_human.csv')\n",
    "shuffledata = dataframe.sample(frac=1)\n",
    "human_conc_rawTarget=GetTargetVector(shuffledata)\n",
    "human_conc_rawData=GenerateRawData(shuffledata,18)\n",
    "# Reading Human Subtraction Data\n",
    "dataframe = pd.read_csv('output_subt_human.csv')\n",
    "shuffleddata = dataframe.sample(frac=1)\n",
    "human_subt_rawTarget=GetTargetVector(shuffledata)\n",
    "human_subt_rawData=GenerateRawData(shuffleddata,9)\n",
    "# Reading GSC Concatenation Data\n",
    "dataframe = pd.read_csv('output_conc_gsc.csv')\n",
    "shuffleddata = dataframe.sample(frac=1)\n",
    "gsc_conc_rawTarget=GetTargetVector(shuffleddata)\n",
    "gsc_conc_rawData=GenerateRawData(shuffleddata,1024)\n",
    "# Reading GSC Subtraction Data\n",
    "dataframe = pd.read_csv('output_subt_gsc.csv')\n",
    "shuffleddata = dataframe.sample(frac=1)\n",
    "gsc_subt_rawTarget=GetTargetVector(shuffleddata)\n",
    "gsc_subt_rawData=GenerateRawData(shuffleddata,512)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generating Training Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1234,)\n",
      "(18, 1234)\n",
      "(1234,)\n",
      "(9, 1234)\n",
      "(16890,)\n",
      "(1017, 16890)\n",
      "(16890,)\n",
      "(509, 16890)\n"
     ]
    }
   ],
   "source": [
    "# Generating Human Concatenation Training Data\n",
    "TrainingPercent=80\n",
    "Human_Conc_TrainingTarget = np.array(GenerateTrainingTarget(human_conc_rawTarget,TrainingPercent))\n",
    "Human_Conc_TrainingTarget = Human_Conc_TrainingTarget.astype(int)\n",
    "Human_Conc_TrainingData   = GenerateTrainingDataMatrix(human_conc_rawData,TrainingPercent)\n",
    "Human_Conc_TrainingData   = Human_Conc_TrainingData.astype(int)\n",
    "print(Human_Conc_TrainingTarget.shape)\n",
    "print(Human_Conc_TrainingData.shape)\n",
    "# Generating Human Subtraction Training Data\n",
    "Human_Subt_TrainingTarget = np.array(GenerateTrainingTarget(human_subt_rawTarget,TrainingPercent))\n",
    "Human_Subt_TrainingTarget = Human_Subt_TrainingTarget.astype(int)\n",
    "Human_Subt_TrainingData   = GenerateTrainingDataMatrix(human_subt_rawData,TrainingPercent)\n",
    "Human_Subt_TrainingData   = Human_Subt_TrainingData.astype(int)\n",
    "print(Human_Subt_TrainingTarget.shape)\n",
    "print(Human_Subt_TrainingData.shape)\n",
    "# Generating GSC Concatenation Training Data \n",
    "Gsc_Conc_TrainingTarget = np.array(GenerateTrainingTarget(gsc_conc_rawTarget,TrainingPercent))\n",
    "Gsc_Conc_TrainingTarget = Gsc_Conc_TrainingTarget.astype(int)\n",
    "Gsc_Conc_TrainingData   = GenerateTrainingDataMatrix(gsc_conc_rawData,TrainingPercent)\n",
    "Gsc_Conc_TrainingData   = Gsc_Conc_TrainingData.astype(int)\n",
    "print(Gsc_Conc_TrainingTarget.shape)\n",
    "print(Gsc_Conc_TrainingData.shape)\n",
    "# Generating GSC Subtraction TrainingData\n",
    "Gsc_Subt_TrainingTarget = np.array(GenerateTrainingTarget(gsc_subt_rawTarget,TrainingPercent))\n",
    "Gsc_Subt_TrainingTarget = Gsc_Subt_TrainingTarget.astype(int)\n",
    "Gsc_Subt_TrainingData   = GenerateTrainingDataMatrix(gsc_subt_rawData,TrainingPercent)\n",
    "Gsc_Subt_TrainingData   = Gsc_Subt_TrainingData.astype(int)\n",
    "print(Gsc_Subt_TrainingTarget.shape)\n",
    "print(Gsc_Subt_TrainingData.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generating Validation Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(154,)\n",
      "(18, 154)\n",
      "(154,)\n",
      "(9, 154)\n",
      "(2111,)\n",
      "(1017, 2111)\n",
      "(2111,)\n",
      "(509, 2111)\n"
     ]
    }
   ],
   "source": [
    "ValidationPercent=10\n",
    "Human_Conc_ValidationTarget = np.array(GenerateValTargetVector(human_conc_rawTarget,ValidationPercent, (len(Human_Conc_TrainingTarget))))\n",
    "Human_Conc_ValidationTarget = Human_Conc_ValidationTarget.astype(int)\n",
    "Human_Conc_ValidationData   = GenerateValData(human_conc_rawData,ValidationPercent, (len(Human_Conc_TrainingTarget)))\n",
    "Human_Conc_ValidationData   = Human_Conc_ValidationData.astype(int)\n",
    "print(Human_Conc_ValidationTarget.shape)\n",
    "print(Human_Conc_ValidationData.shape)\n",
    "# Generating Human Subtraction Training Data\n",
    "Human_Subt_ValidationTarget = np.array(GenerateValTargetVector(human_subt_rawTarget,ValidationPercent, (len(Human_Subt_TrainingTarget))))\n",
    "Human_Subt_ValidationTarget = Human_Subt_ValidationTarget.astype(int)\n",
    "Human_Subt_ValidationData   = GenerateValData(human_subt_rawData,ValidationPercent, (len(Human_Subt_TrainingTarget)))\n",
    "Human_Subt_ValidationData   = Human_Subt_ValidationData.astype(int)\n",
    "print(Human_Subt_ValidationTarget.shape)\n",
    "print(Human_Subt_ValidationData.shape)\n",
    "# Generating GSC Concatenation Training Data \n",
    "Gsc_Conc_ValidationTarget = np.array(GenerateValTargetVector(gsc_conc_rawTarget,ValidationPercent, (len(Gsc_Conc_TrainingTarget))))\n",
    "Gsc_Conc_ValidationTarget = Gsc_Conc_ValidationTarget.astype(int)\n",
    "Gsc_Conc_ValidationData   = GenerateValData(gsc_conc_rawData,ValidationPercent, (len(Gsc_Conc_TrainingTarget)))\n",
    "Gsc_Conc_ValidationData   = Gsc_Conc_ValidationData.astype(int)\n",
    "print(Gsc_Conc_ValidationTarget.shape)\n",
    "print(Gsc_Conc_ValidationData.shape)\n",
    "# Generating GSC Subtraction TrainingData\n",
    "Gsc_Subt_ValidationTarget = np.array(GenerateValTargetVector(gsc_subt_rawTarget,ValidationPercent, (len(Gsc_Subt_TrainingTarget))))\n",
    "Gsc_Subt_ValidationTarget = Gsc_Subt_ValidationTarget.astype(int)\n",
    "Gsc_Subt_ValidationData   = GenerateValData(gsc_subt_rawData,ValidationPercent, (len(Gsc_Subt_TrainingTarget)))\n",
    "Gsc_Subt_ValidationData   = Gsc_Subt_ValidationData.astype(int)\n",
    "print(Gsc_Subt_ValidationTarget.shape)\n",
    "print(Gsc_Subt_ValidationData.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generating Testing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(153,)\n",
      "(18, 153)\n",
      "(153,)\n",
      "(9, 153)\n",
      "(2110,)\n",
      "(1017, 2110)\n",
      "(2110,)\n",
      "(509, 2110)\n"
     ]
    }
   ],
   "source": [
    "ValidationPercent=10\n",
    "# Generating Human Concatenation Training Data\n",
    "Human_Conc_TestingTarget = np.array(GenerateValTargetVector(human_conc_rawTarget,ValidationPercent, (len(Human_Conc_TrainingTarget)+len(Human_Conc_ValidationTarget))))\n",
    "Human_Conc_TestingTarget = Human_Conc_TestingTarget.astype(int)\n",
    "Human_Conc_TestingData   = GenerateValData(human_conc_rawData,ValidationPercent, (len(Human_Conc_TrainingTarget)+len(Human_Conc_ValidationTarget)))\n",
    "Human_Conc_TestingData   = Human_Conc_TestingData.astype(int)\n",
    "print(Human_Conc_TestingTarget.shape)\n",
    "print(Human_Conc_TestingData.shape)\n",
    "# Generating Human Subtraction Training Data\n",
    "Human_Subt_TestingTarget = np.array(GenerateValTargetVector(human_subt_rawTarget,ValidationPercent, (len(Human_Subt_TrainingTarget)+len(Human_Subt_ValidationTarget))))\n",
    "Human_Subt_TestingTarget = Human_Subt_TestingTarget.astype(int)\n",
    "Human_Subt_TestingData   = GenerateValData(human_subt_rawData,ValidationPercent, (len(Human_Subt_TrainingTarget)+len(Human_Subt_ValidationTarget)))\n",
    "Human_Subt_TestingData   = Human_Subt_TestingData.astype(int)\n",
    "print(Human_Subt_TestingTarget.shape)\n",
    "print(Human_Subt_TestingData.shape)\n",
    "# Generating GSC Concatenation Training Data \n",
    "Gsc_Conc_TestingTarget = np.array(GenerateValTargetVector(gsc_conc_rawTarget,ValidationPercent, (len(Gsc_Conc_TrainingTarget)+len(Gsc_Conc_ValidationTarget))))\n",
    "Gsc_Conc_TestingTarget = Gsc_Conc_TestingTarget.astype(int)\n",
    "Gsc_Conc_TestingData   = GenerateValData(gsc_conc_rawData,ValidationPercent,(len(Gsc_Conc_TrainingTarget)+len(Gsc_Conc_ValidationTarget)))\n",
    "Gsc_Conc_TestingData   = Gsc_Conc_TestingData.astype(int)\n",
    "print(Gsc_Conc_TestingTarget.shape)\n",
    "print(Gsc_Conc_TestingData.shape)\n",
    "# Generating GSC Subtraction TrainingData\n",
    "Gsc_Subt_TestingTarget = np.array(GenerateValTargetVector(gsc_subt_rawTarget,ValidationPercent, (len(Gsc_Subt_TrainingTarget)+len(Gsc_Subt_ValidationTarget))))\n",
    "Gsc_Subt_TestingTarget = Gsc_Subt_TestingTarget.astype(int)\n",
    "Gsc_Subt_TestingData   = GenerateValData(gsc_subt_rawData,ValidationPercent, (len(Gsc_Subt_TrainingTarget)+len(Gsc_Subt_ValidationTarget)))\n",
    "Gsc_Subt_TestingData   = Gsc_Subt_TestingData.astype(int)\n",
    "print(Gsc_Subt_TestingTarget.shape)\n",
    "print(Gsc_Subt_TestingData.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#               Linear Regression Using Stochastic Gradient Descent Method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Human Observed Concatenation Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "ErmsArr = []\n",
    "AccuracyArr = []\n",
    "M=1\n",
    "learningRate=0.001\n",
    "# Calculating k means cluster to design Mu matrix equal to number of basis function\n",
    "kmeans = KMeans(n_clusters=M, random_state=0).fit(np.transpose(Human_Conc_TrainingData))\n",
    "Mu = kmeans.cluster_centers_\n",
    "BigSigma     = GenerateBigSigma(Human_Conc_TrainingData,100)\n",
    "TRAINING_PHI = GetPhiMatrix(human_conc_rawData, Mu, BigSigma, TrainingPercent)\n",
    "TEST_PHI     = GetPhiMatrix(Human_Conc_TestingData, Mu, BigSigma, 100) \n",
    "VAL_PHI      = GetPhiMatrix(Human_Conc_ValidationData, Mu, BigSigma, 100) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "W_T_next=linearregression(Human_Conc_TrainingTarget,TRAINING_PHI,learningRate,1000)\n",
    "L_Erms_Val   = []\n",
    "L_Erms_TR    = []\n",
    "L_Erms_Test  = []\n",
    "    \n",
    "    #-----------------TrainingData Accuracy---------------------#\n",
    "TR_TEST_OUT   = GetValTest(TRAINING_PHI,W_T_next) \n",
    "Erms_TR       = GetErms(TR_TEST_OUT,Human_Conc_TrainingTarget)\n",
    "L_Erms_TR.append(float(Erms_TR.split(',')[1]))\n",
    "    \n",
    "    #-----------------ValidationData Accuracy---------------------#\n",
    "VAL_TEST_OUT  = GetValTest(VAL_PHI,W_T_next) \n",
    "Erms_Val      = GetErms(VAL_TEST_OUT,Human_Conc_ValidationTarget)\n",
    "L_Erms_Val.append(float(Erms_Val.split(',')[1]))\n",
    "    \n",
    "    #-----------------TestingData Accuracy---------------------#\n",
    "TEST_OUT      = GetValTest(TEST_PHI,W_T_next) \n",
    "Erms_Test = GetErms(TEST_OUT,Human_Conc_TestingTarget)\n",
    "L_Erms_Test.append(float(Erms_Test.split(',')[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UBITname      = mishra3\n",
      "Person Number = 50290757\n",
      "------------------Linear Regression on Human Observed Concatenation Dataset--------------------\n",
      "E_rms Training   = 0.71847\n",
      "E_rms Validation = 0.7025\n",
      "E_rms Testing    = 0.70941\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#print(str(L_Erms_TR))\n",
    "print (\"E_rms Training   = \" + str(np.around(max(L_Erms_TR),5)))\n",
    "print (\"E_rms Validation = \" + str(np.around(max(L_Erms_Val),5)))\n",
    "print (\"E_rms Testing    = \" + str(np.around(max(L_Erms_Test),5)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Human Observed Subtraction Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "ErmsArr = []\n",
    "AccuracyArr = []\n",
    "M=1\n",
    "learningRate=0.001\n",
    "# Calculating k means cluster to design Mu matrix equal to number of basis function\n",
    "kmeans = KMeans(n_clusters=M, random_state=0).fit(np.transpose(Human_Subt_TrainingData))\n",
    "Mu = kmeans.cluster_centers_\n",
    "BigSigma     = GenerateBigSigma(Human_Subt_TrainingData,100)\n",
    "TRAINING_PHI = GetPhiMatrix(human_subt_rawData, Mu, BigSigma, TrainingPercent)\n",
    "TEST_PHI     = GetPhiMatrix(Human_Subt_TestingData, Mu, BigSigma, 100) \n",
    "VAL_PHI      = GetPhiMatrix(Human_Subt_ValidationData, Mu, BigSigma, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "W_T_next=linearregression(Human_Subt_TrainingTarget,TRAINING_PHI,learningRate,1000)\n",
    "L_Erms_Val   = []\n",
    "L_Erms_TR    = []\n",
    "L_Erms_Test  = []   \n",
    "    #-----------------TrainingData Accuracy---------------------#\n",
    "TR_TEST_OUT   = GetValTest(TRAINING_PHI,W_T_next) \n",
    "Erms_TR       = GetErms(TR_TEST_OUT,Human_Subt_TrainingTarget)\n",
    "L_Erms_TR.append(float(Erms_TR.split(',')[1]))\n",
    "    \n",
    "    #-----------------ValidationData Accuracy---------------------#\n",
    "VAL_TEST_OUT  = GetValTest(VAL_PHI,W_T_next) \n",
    "Erms_Val      = GetErms(VAL_TEST_OUT,Human_Subt_ValidationTarget)\n",
    "L_Erms_Val.append(float(Erms_Val.split(',')[1]))\n",
    "    \n",
    "    #-----------------TestingData Accuracy---------------------#\n",
    "TEST_OUT      = GetValTest(TEST_PHI,W_T_next) \n",
    "Erms_Test = GetErms(TEST_OUT,Human_Subt_TestingTarget)\n",
    "L_Erms_Test.append(float(Erms_Test.split(',')[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------Linear Regression on Human Observed Subtraction Dataset--------------------\n",
      "E_rms Training   = 0.71833\n",
      "E_rms Validation = 0.70232\n",
      "E_rms Testing    = 0.7093\n"
     ]
    }
   ],
   "source": [
    "print ('------------------Linear Regression on Human Observed Subtraction Dataset--------------------')\n",
    "#print(str(L_Erms_TR))\n",
    "print (\"E_rms Training   = \" + str(np.around(max(L_Erms_TR),5)))\n",
    "print (\"E_rms Validation = \" + str(np.around(max(L_Erms_Val),5)))\n",
    "print (\"E_rms Testing    = \" + str(np.around(max(L_Erms_Test),5)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GSC Concatenation Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "ErmsArr = []\n",
    "AccuracyArr = []\n",
    "M=1\n",
    "learningRate=0.01\n",
    "# Calculating k means cluster to design Mu matrix equal to number of basis function\n",
    "kmeans = KMeans(n_clusters=M, random_state=0).fit(np.transpose(Gsc_Conc_TrainingData))\n",
    "Mu = kmeans.cluster_centers_\n",
    "BigSigma     = GenerateBigSigma(Gsc_Conc_TrainingData,100)\n",
    "TRAINING_PHI = GetPhiMatrix(gsc_conc_rawData, Mu, BigSigma, TrainingPercent)\n",
    "TEST_PHI     = GetPhiMatrix(Gsc_Conc_TestingData, Mu, BigSigma, 100) \n",
    "VAL_PHI      = GetPhiMatrix(Gsc_Conc_ValidationData, Mu, BigSigma, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "W_T_next=linearregression(Gsc_Conc_TrainingTarget,TRAINING_PHI,learningRate,10000)\n",
    "L_Erms_Val   = []\n",
    "L_Erms_TR    = []\n",
    "L_Erms_Test  = []   \n",
    "    #-----------------TrainingData Accuracy---------------------#\n",
    "TR_TEST_OUT   = GetValTest(TRAINING_PHI,W_T_next) \n",
    "Erms_TR       = GetErms(TR_TEST_OUT,Gsc_Conc_TrainingTarget)\n",
    "L_Erms_TR.append(float(Erms_TR.split(',')[1]))\n",
    "    \n",
    "    #-----------------ValidationData Accuracy---------------------#\n",
    "VAL_TEST_OUT  = GetValTest(VAL_PHI,W_T_next) \n",
    "Erms_Val      = GetErms(VAL_TEST_OUT,Gsc_Conc_ValidationTarget)\n",
    "L_Erms_Val.append(float(Erms_Val.split(',')[1]))\n",
    "    \n",
    "    #-----------------TestingData Accuracy---------------------#\n",
    "TEST_OUT      = GetValTest(TEST_PHI,W_T_next) \n",
    "Erms_Test = GetErms(TEST_OUT,Gsc_Conc_TestingTarget)\n",
    "L_Erms_Test.append(float(Erms_Test.split(',')[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------Linear Regression on GSC concatenation Dataset--------------------\n",
      "E_rms Training   = 0.69656\n",
      "E_rms Validation = 0.6975\n",
      "E_rms Testing    = 0.68705\n"
     ]
    }
   ],
   "source": [
    "print ('------------------Linear Regression on GSC concatenation Dataset--------------------')\n",
    "#print(str(L_Erms_TR))\n",
    "print (\"E_rms Training   = \" + str(np.around(max(L_Erms_TR),5)))\n",
    "print (\"E_rms Validation = \" + str(np.around(max(L_Erms_Val),5)))\n",
    "print (\"E_rms Testing    = \" + str(np.around(max(L_Erms_Test),5)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GSC Subtraction Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "ErmsArr = []\n",
    "AccuracyArr = []\n",
    "# Calculating k means cluster to design Mu matrix equal to number of basis function\n",
    "M=1\n",
    "learningRate=0.05\n",
    "kmeans = KMeans(n_clusters=M, random_state=0).fit(np.transpose(Gsc_Subt_TrainingData))\n",
    "Mu = kmeans.cluster_centers_\n",
    "BigSigma     = GenerateBigSigma(Gsc_Subt_TrainingData,100)\n",
    "TRAINING_PHI = GetPhiMatrix(gsc_subt_rawData, Mu, BigSigma, TrainingPercent)\n",
    "TEST_PHI     = GetPhiMatrix(Gsc_Subt_TestingData, Mu, BigSigma, 100) \n",
    "VAL_PHI      = GetPhiMatrix(Gsc_Subt_ValidationData, Mu, BigSigma, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "W_T_next=linearregression(Gsc_Subt_TrainingTarget,TRAINING_PHI,learningRate,5000)\n",
    "L_Erms_Val   = []\n",
    "L_Erms_TR    = []\n",
    "L_Erms_Test  = []   \n",
    "    #-----------------TrainingData Accuracy---------------------#\n",
    "TR_TEST_OUT   = GetValTest(TRAINING_PHI,W_T_next) \n",
    "Erms_TR       = GetErms(TR_TEST_OUT,Gsc_Subt_TrainingTarget)\n",
    "L_Erms_TR.append(float(Erms_TR.split(',')[1]))\n",
    "    \n",
    "    #-----------------ValidationData Accuracy---------------------#\n",
    "VAL_TEST_OUT  = GetValTest(VAL_PHI,W_T_next) \n",
    "Erms_Val      = GetErms(VAL_TEST_OUT,Gsc_Subt_ValidationTarget)\n",
    "L_Erms_Val.append(float(Erms_Val.split(',')[1]))\n",
    "    \n",
    "    #-----------------TestingData Accuracy---------------------#\n",
    "TEST_OUT      = GetValTest(TEST_PHI,W_T_next) \n",
    "Erms_Test = GetErms(TEST_OUT,Gsc_Subt_TestingTarget)\n",
    "L_Erms_Test.append(float(Erms_Test.split(',')[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------Linear Regression on GSC subtraction Dataset--------------------\n",
      "E_rms Training   = 0.6966\n",
      "E_rms Validation = 0.70156\n",
      "E_rms Testing    = 0.6822\n"
     ]
    }
   ],
   "source": [
    "print ('------------------Linear Regression on GSC subtraction Dataset--------------------')\n",
    "#print(str(L_Erms_TR))\n",
    "print (\"E_rms Training   = \" + str(np.around(max(L_Erms_TR),5)))\n",
    "print (\"E_rms Validation = \" + str(np.around(max(L_Erms_Val),5)))\n",
    "print (\"E_rms Testing    = \" + str(np.around(max(L_Erms_Test),5)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GetTarget(shuffledata):\n",
    "    t = []\n",
    "    for i in range(0,len(shuffledata)):\n",
    "            t.append(shuffledata.iloc[i,2])\n",
    "    #print(\"Raw Training Generated..\")\n",
    "    return np.array(t)\n",
    "def GenerateData(shuffledata,featurelength):    \n",
    "    dataMatrix = []\n",
    "    for i in range(0,len(shuffledata)):\n",
    "        dataMatrix.append(shuffledata.iloc[i,3:featurelength+3])\n",
    "    dataMatrix=np.transpose(dataMatrix).astype(int) \n",
    "    return dataMatrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading Human Concatenation Data\n",
    "dataframe = pd.read_csv('output_conc_human.csv')\n",
    "shuffledata = dataframe.sample(frac=1)\n",
    "human_conc_rawTarget=GetTarget(shuffledata)\n",
    "human_conc_rawData=GenerateData(shuffledata,18)\n",
    "# Reading Human Subtraction Data\n",
    "dataframe = pd.read_csv('output_subt_human.csv')\n",
    "shuffleddata = dataframe.sample(frac=1)\n",
    "human_subt_rawTarget=GetTarget(shuffledata)\n",
    "human_subt_rawData=GenerateData(shuffleddata,9)\n",
    "# Reading GSC Concatenation Data\n",
    "dataframe = pd.read_csv('output_conc_gsc.csv')\n",
    "shuffleddata = dataframe.sample(frac=1)\n",
    "gsc_conc_rawTarget=GetTarget(shuffleddata)\n",
    "gsc_conc_rawData=GenerateData(shuffleddata,1024)\n",
    "# Reading GSC Subtraction Data\n",
    "dataframe = pd.read_csv('output_subt_gsc.csv')\n",
    "shuffleddata = dataframe.sample(frac=1)\n",
    "gsc_subt_rawTarget=GetTarget(shuffleddata)\n",
    "gsc_subt_rawData=GenerateData(shuffleddata,512)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generating Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1234,)\n",
      "(18, 1234)\n",
      "(1234,)\n",
      "(9, 1234)\n",
      "(16890,)\n",
      "(1024, 16890)\n",
      "(16890,)\n",
      "(512, 16890)\n"
     ]
    }
   ],
   "source": [
    "# Generating Human Concatenation Training Data\n",
    "TrainingPercent=80\n",
    "Human_Conc_TrainingTarget = np.array(GenerateTrainingTarget(human_conc_rawTarget,TrainingPercent))\n",
    "Human_Conc_TrainingTarget = Human_Conc_TrainingTarget.astype(int)\n",
    "Human_Conc_TrainingData   = GenerateTrainingDataMatrix(human_conc_rawData,TrainingPercent)\n",
    "Human_Conc_TrainingData   = Human_Conc_TrainingData.astype(int)\n",
    "print(Human_Conc_TrainingTarget.shape)\n",
    "print(Human_Conc_TrainingData.shape)\n",
    "# Generating Human Subtraction Training Data\n",
    "Human_Subt_TrainingTarget = np.array(GenerateTrainingTarget(human_subt_rawTarget,TrainingPercent))\n",
    "Human_Subt_TrainingTarget = Human_Subt_TrainingTarget.astype(int)\n",
    "Human_Subt_TrainingData   = GenerateTrainingDataMatrix(human_subt_rawData,TrainingPercent)\n",
    "Human_Subt_TrainingData   = Human_Subt_TrainingData.astype(int)\n",
    "print(Human_Subt_TrainingTarget.shape)\n",
    "print(Human_Subt_TrainingData.shape)\n",
    "# Generating GSC Concatenation Training Data \n",
    "Gsc_Conc_TrainingTarget = np.array(GenerateTrainingTarget(gsc_conc_rawTarget,TrainingPercent))\n",
    "Gsc_Conc_TrainingTarget = Gsc_Conc_TrainingTarget.astype(int)\n",
    "Gsc_Conc_TrainingData   = GenerateTrainingDataMatrix(gsc_conc_rawData,TrainingPercent)\n",
    "Gsc_Conc_TrainingData   = Gsc_Conc_TrainingData.astype(int)\n",
    "print(Gsc_Conc_TrainingTarget.shape)\n",
    "print(Gsc_Conc_TrainingData.shape)\n",
    "# Generating GSC Subtraction TrainingData\n",
    "Gsc_Subt_TrainingTarget = np.array(GenerateTrainingTarget(gsc_subt_rawTarget,TrainingPercent))\n",
    "Gsc_Subt_TrainingTarget = Gsc_Subt_TrainingTarget.astype(int)\n",
    "Gsc_Subt_TrainingData   = GenerateTrainingDataMatrix(gsc_subt_rawData,TrainingPercent)\n",
    "Gsc_Subt_TrainingData   = Gsc_Subt_TrainingData.astype(int)\n",
    "print(Gsc_Subt_TrainingTarget.shape)\n",
    "print(Gsc_Subt_TrainingData.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generating Validation Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(154,)\n",
      "(18, 154)\n",
      "(154,)\n",
      "(9, 154)\n",
      "(2111,)\n",
      "(1024, 2111)\n",
      "(2111,)\n",
      "(512, 2111)\n"
     ]
    }
   ],
   "source": [
    "ValidationPercent=10\n",
    "Human_Conc_ValidationTarget = np.array(GenerateValTargetVector(human_conc_rawTarget,ValidationPercent, (len(Human_Conc_TrainingTarget))))\n",
    "Human_Conc_ValidationTarget = Human_Conc_ValidationTarget.astype(int)\n",
    "Human_Conc_ValidationData   = GenerateValData(human_conc_rawData,ValidationPercent, (len(Human_Conc_TrainingTarget)))\n",
    "Human_Conc_ValidationData   = Human_Conc_ValidationData.astype(int)\n",
    "print(Human_Conc_ValidationTarget.shape)\n",
    "print(Human_Conc_ValidationData.shape)\n",
    "# Generating Human Subtraction Training Data\n",
    "Human_Subt_ValidationTarget = np.array(GenerateValTargetVector(human_subt_rawTarget,ValidationPercent, (len(Human_Subt_TrainingTarget))))\n",
    "Human_Subt_ValidationTarget = Human_Subt_ValidationTarget.astype(int)\n",
    "Human_Subt_ValidationData   = GenerateValData(human_subt_rawData,ValidationPercent, (len(Human_Subt_TrainingTarget)))\n",
    "Human_Subt_ValidationData   = Human_Subt_ValidationData.astype(int)\n",
    "print(Human_Subt_ValidationTarget.shape)\n",
    "print(Human_Subt_ValidationData.shape)\n",
    "# Generating GSC Concatenation Training Data \n",
    "Gsc_Conc_ValidationTarget = np.array(GenerateValTargetVector(gsc_conc_rawTarget,ValidationPercent, (len(Gsc_Conc_TrainingTarget))))\n",
    "Gsc_Conc_ValidationTarget = Gsc_Conc_ValidationTarget.astype(int)\n",
    "Gsc_Conc_ValidationData   = GenerateValData(gsc_conc_rawData,ValidationPercent, (len(Gsc_Conc_TrainingTarget)))\n",
    "Gsc_Conc_ValidationData   = Gsc_Conc_ValidationData.astype(int)\n",
    "print(Gsc_Conc_ValidationTarget.shape)\n",
    "print(Gsc_Conc_ValidationData.shape)\n",
    "# Generating GSC Subtraction TrainingData\n",
    "Gsc_Subt_ValidationTarget = np.array(GenerateValTargetVector(gsc_subt_rawTarget,ValidationPercent, (len(Gsc_Subt_TrainingTarget))))\n",
    "Gsc_Subt_ValidationTarget = Gsc_Subt_ValidationTarget.astype(int)\n",
    "Gsc_Subt_ValidationData   = GenerateValData(gsc_subt_rawData,ValidationPercent, (len(Gsc_Subt_TrainingTarget)))\n",
    "Gsc_Subt_ValidationData   = Gsc_Subt_ValidationData.astype(int)\n",
    "print(Gsc_Subt_ValidationTarget.shape)\n",
    "print(Gsc_Subt_ValidationData.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generating Testing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(153,)\n",
      "(18, 153)\n",
      "(153,)\n",
      "(9, 153)\n",
      "(2110,)\n",
      "(1024, 2110)\n",
      "(2110,)\n",
      "(512, 2110)\n"
     ]
    }
   ],
   "source": [
    "ValidationPercent=10\n",
    "# Generating Human Concatenation Training Data\n",
    "Human_Conc_TestingTarget = np.array(GenerateValTargetVector(human_conc_rawTarget,ValidationPercent, (len(Human_Conc_TrainingTarget)+len(Human_Conc_ValidationTarget))))\n",
    "Human_Conc_TestingTarget = Human_Conc_TestingTarget.astype(int)\n",
    "Human_Conc_TestingData   = GenerateValData(human_conc_rawData,ValidationPercent, (len(Human_Conc_TrainingTarget)+len(Human_Conc_ValidationTarget)))\n",
    "Human_Conc_TestingData   = Human_Conc_TestingData.astype(int)\n",
    "print(Human_Conc_TestingTarget.shape)\n",
    "print(Human_Conc_TestingData.shape)\n",
    "# Generating Human Subtraction Training Data\n",
    "Human_Subt_TestingTarget = np.array(GenerateValTargetVector(human_subt_rawTarget,ValidationPercent, (len(Human_Subt_TrainingTarget)+len(Human_Subt_ValidationTarget))))\n",
    "Human_Subt_TestingTarget = Human_Subt_TestingTarget.astype(int)\n",
    "Human_Subt_TestingData   = GenerateValData(human_subt_rawData,ValidationPercent, (len(Human_Subt_TrainingTarget)+len(Human_Subt_ValidationTarget)))\n",
    "Human_Subt_TestingData   = Human_Subt_TestingData.astype(int)\n",
    "print(Human_Subt_TestingTarget.shape)\n",
    "print(Human_Subt_TestingData.shape)\n",
    "# Generating GSC Concatenation Training Data \n",
    "Gsc_Conc_TestingTarget = np.array(GenerateValTargetVector(gsc_conc_rawTarget,ValidationPercent, (len(Gsc_Conc_TrainingTarget)+len(Gsc_Conc_ValidationTarget))))\n",
    "Gsc_Conc_TestingTarget = Gsc_Conc_TestingTarget.astype(int)\n",
    "Gsc_Conc_TestingData   = GenerateValData(gsc_conc_rawData,ValidationPercent,(len(Gsc_Conc_TrainingTarget)+len(Gsc_Conc_ValidationTarget)))\n",
    "Gsc_Conc_TestingData   = Gsc_Conc_TestingData.astype(int)\n",
    "print(Gsc_Conc_TestingTarget.shape)\n",
    "print(Gsc_Conc_TestingData.shape)\n",
    "# Generating GSC Subtraction TrainingData\n",
    "Gsc_Subt_TestingTarget = np.array(GenerateValTargetVector(gsc_subt_rawTarget,ValidationPercent, (len(Gsc_Subt_TrainingTarget)+len(Gsc_Subt_ValidationTarget))))\n",
    "Gsc_Subt_TestingTarget = Gsc_Subt_TestingTarget.astype(int)\n",
    "Gsc_Subt_TestingData   = GenerateValData(gsc_subt_rawData,ValidationPercent, (len(Gsc_Subt_TrainingTarget)+len(Gsc_Subt_ValidationTarget)))\n",
    "Gsc_Subt_TestingData   = Gsc_Subt_TestingData.astype(int)\n",
    "print(Gsc_Subt_TestingTarget.shape)\n",
    "print(Gsc_Subt_TestingData.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating the Accuracy\n",
    "def calaccuracy(Y,TrainingTarget):\n",
    "    count=0\n",
    "    for i in range(0,len(Y)):\n",
    "        # If expected output is less than 0.5 then it belong to different writer\n",
    "        if(Y[i]<=0.5):\n",
    "            if(TrainingTarget[i]==0):\n",
    "              count=count+1\n",
    "        else:\n",
    "            if(TrainingTarget[i]==1):\n",
    "               count=count+1\n",
    "    return (count/len(TrainingTarget))*100\n",
    "# Calculating the Sigmoid\n",
    "def sigmoid(z):\n",
    "    return 1/(1+np.exp(-z))\n",
    "# Calculating the genatic equation\n",
    "def geneticequation(W,X):\n",
    "    return np.dot(W,np.transpose(X))\n",
    "# Calculating the loss function (cross entropy)\n",
    "def lossfunction(a,y):\n",
    "    t=np.dot(-np.transpose(y),np.log(a))\n",
    "    u=np.dot(np.transpose(1-y),np.log(1-a))\n",
    "    return (np.subtract(t,u)).mean()\n",
    "# Calculating the weight derivative\n",
    "def weightderivative(a,y,X):\n",
    "    return np.dot(np.subtract(a,y),X)\n",
    "# Updating the weigth\n",
    "def updatedweigth(lr,dw,W):\n",
    "    return np.subtract(W,lr*dw)\n",
    "# Performing logistic regression\n",
    "def logisticregression(learningrate,TrainingData,TrainingTarget,iteration):\n",
    "    # Intializing the weigths\n",
    "    W_Now        = np.dot(0,np.zeros(len(np.transpose(TrainingData))))\n",
    "    learningrate=0.01\n",
    "    for i in range(0,iteration):\n",
    "        Y=geneticequation(W_Now,TrainingData[i])\n",
    "        a=sigmoid(Y)\n",
    "        L=lossfunction(a,TrainingTarget)\n",
    "        deltaW=weightderivative(a,TrainingTarget[i],TrainingData[i])\n",
    "        W_Now=updatedweigth(learningrate,deltaW,W_Now)\n",
    "    return W_Now\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Human Concatenation DataSet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "learningrate=0.01\n",
    "W_Now=logisticregression(learningrate,np.transpose(Human_Conc_TrainingData),Human_Conc_TrainingTarget,800)\n",
    "# Calculating Training Accuracy\n",
    "Y=geneticequation(W_Now,np.transpose(Human_Conc_TrainingData))\n",
    "b=sigmoid(Y)\n",
    "TrainingAccuracy=calaccuracy(b,Human_Conc_TrainingTarget)\n",
    "# Calculating Validation Accuracy\n",
    "Y=geneticequation(W_Now,np.transpose(Human_Conc_ValidationData))\n",
    "b=sigmoid(Y)\n",
    "ValidationAccuracy=calaccuracy(b,Human_Conc_ValidationTarget)\n",
    "# Calculating Testing Accuracy\n",
    "Y=geneticequation(W_Now,np.transpose(Human_Conc_TestingData))\n",
    "b=sigmoid(Y)\n",
    "TestingAccuracy=calaccuracy(b,Human_Conc_TestingTarget)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------Logistic Regression on Human Concatenation Dataset--------------------\n",
      "Training Accuracy  = 48.21717990275527\n",
      "Validation Accuracy  = 53.246753246753244\n",
      "Testing Accuracy  = 49.01960784313725\n"
     ]
    }
   ],
   "source": [
    "print ('------------------Logistic Regression on Human Concatenation Dataset--------------------')\n",
    "#print(str(L_Erms_TR))\n",
    "print (\"Training Accuracy  = \" + str(TrainingAccuracy))\n",
    "print (\"Validation Accuracy  = \" + str(ValidationAccuracy))\n",
    "print (\"Testing Accuracy  = \" + str(TestingAccuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Human Subtraction Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "learningrate=0.01\n",
    "W_Now=logisticregression(learningrate,np.transpose(Human_Subt_TrainingData),Human_Subt_TrainingTarget,1200)\n",
    "# Calculating Training Accuracy\n",
    "Y=geneticequation(W_Now,np.transpose(Human_Subt_TrainingData))\n",
    "b=sigmoid(Y)\n",
    "TrainingAccuracy=calaccuracy(b,Human_Subt_TrainingTarget)\n",
    "# Calculating Validation Accuracy\n",
    "Y=geneticequation(W_Now,np.transpose(Human_Subt_ValidationData))\n",
    "b=sigmoid(Y)\n",
    "ValidationAccuracy=calaccuracy(b,Human_Subt_ValidationTarget)\n",
    "# Calculating Testing Accuracy\n",
    "Y=geneticequation(W_Now,np.transpose(Human_Subt_TestingData))\n",
    "b=sigmoid(Y)\n",
    "TestingAccuracy=calaccuracy(b,Human_Subt_TestingTarget)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------Logistic Regression on Human Subtraction Dataset--------------------\n",
      "Training Accuracy  = 51.944894651539705\n",
      "Validation Accuracy  = 41.55844155844156\n",
      "Testing Accuracy  = 43.13725490196079\n"
     ]
    }
   ],
   "source": [
    "print ('------------------Logistic Regression on Human Subtraction Dataset--------------------')\n",
    "#print(str(L_Erms_TR))\n",
    "print (\"Training Accuracy  = \" + str(TrainingAccuracy))\n",
    "print (\"Validation Accuracy  = \" + str(ValidationAccuracy))\n",
    "print (\"Testing Accuracy  = \" + str(TestingAccuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gsc Concatenation Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "learningrate=0.01\n",
    "W_Now=logisticregression(learningrate,np.transpose(Gsc_Conc_TrainingData),Gsc_Conc_TrainingTarget,16000)\n",
    "# Calculating Training Accuracy\n",
    "Y=geneticequation(W_Now,np.transpose(Gsc_Conc_TrainingData))\n",
    "b=sigmoid(Y)\n",
    "TrainingAccuracy=calaccuracy(b,Gsc_Conc_TrainingTarget)\n",
    "# Calculating Validation Accuracy\n",
    "Y=geneticequation(W_Now,np.transpose(Gsc_Conc_ValidationData))\n",
    "b=sigmoid(Y)\n",
    "ValidationAccuracy=calaccuracy(b,Gsc_Conc_ValidationTarget)\n",
    "# Calculating Testing Accuracy\n",
    "Y=geneticequation(W_Now,np.transpose(Gsc_Conc_TestingData))\n",
    "b=sigmoid(Y)\n",
    "TestingAccuracy=calaccuracy(b,Gsc_Conc_TestingTarget)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------Logistic Regression on Gsc Concatenation Dataset--------------------\n",
      "Training Accuracy  = 55.98579040852576\n",
      "Validation Accuracy  = 54.57129322595926\n",
      "Testing Accuracy  = 53.459715639810426\n"
     ]
    }
   ],
   "source": [
    "print ('------------------Logistic Regression on Gsc Concatenation Dataset--------------------')\n",
    "#print(str(L_Erms_TR))\n",
    "print (\"Training Accuracy  = \" + str(TrainingAccuracy))\n",
    "print (\"Validation Accuracy  = \" + str(ValidationAccuracy))\n",
    "print (\"Testing Accuracy  = \" + str(TestingAccuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GSC Subtraction Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "learningrate=0.90\n",
    "W_Now=logisticregression(learningrate,np.transpose(Gsc_Subt_TrainingData),Gsc_Subt_TrainingTarget,16000)\n",
    "# Calculating Training Accuracy\n",
    "Y=geneticequation(W_Now,np.transpose(Gsc_Subt_TrainingData))\n",
    "b=sigmoid(Y)\n",
    "TrainingAccuracy=calaccuracy(b,Gsc_Subt_TrainingTarget)\n",
    "# Calculating Validation Accuracy\n",
    "Y=geneticequation(W_Now,np.transpose(Gsc_Subt_ValidationData))\n",
    "b=sigmoid(Y)\n",
    "ValidationAccuracy=calaccuracy(b,Gsc_Subt_ValidationTarget)\n",
    "# Calculating Testing Accuracy\n",
    "Y=geneticequation(W_Now,np.transpose(Gsc_Subt_TestingData))\n",
    "b=sigmoid(Y)\n",
    "TestingAccuracy=calaccuracy(b,Gsc_Subt_TestingTarget)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------Logistic Regression on Gsc Subtraction Dataset--------------------\n",
      "Training Accuracy  = 71.62226169330965\n",
      "Validation Accuracy  = 69.77735670298438\n",
      "Testing Accuracy  = 70.75829383886256\n"
     ]
    }
   ],
   "source": [
    "print ('------------------Logistic Regression on Gsc Subtraction Dataset--------------------')\n",
    "#print(str(L_Erms_TR))\n",
    "print (\"Training Accuracy  = \" + str(TrainingAccuracy))\n",
    "print (\"Validation Accuracy  = \" + str(ValidationAccuracy))\n",
    "print (\"Testing Accuracy  = \" + str(TestingAccuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Human Concatenation Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calaccuracy(Y,TrainingTarget):\n",
    "    count=0\n",
    "    for i in range(0,len(Y)):\n",
    "        if(Y[i]<=0.5):\n",
    "           if(TrainingTarget[i]==0):\n",
    "              count=count+1\n",
    "        else:\n",
    "           if(TrainingTarget[i]==1):\n",
    "              count=count+1\n",
    "    return (count/len(TrainingTarget))*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Intializing the input layer nodes and hiddenlayer nodes and output layer nodes\n",
    "inputlayer=18\n",
    "hiddenlayer=10\n",
    "outputlayer=1\n",
    "LEARNING_RATE = 0.05\n",
    "# Creating the input and output tensor\n",
    "inputTensor=tf.placeholder(tf.float32,[None,inputlayer])\n",
    "outputTensor=tf.placeholder(tf.float32,[None,outputlayer])\n",
    "def init_weights(shape):\n",
    "        return tf.Variable(tf.random_normal(shape,stddev=0.01))\n",
    "input_hidden_weights  = init_weights([inputlayer, hiddenlayer])\n",
    "hidden_output_weights = init_weights([hiddenlayer, outputlayer])\n",
    "hidden_layer = tf.nn.sigmoid(tf.matmul(inputTensor, input_hidden_weights))\n",
    "output_layer = tf.matmul(hidden_layer, hidden_output_weights)\n",
    "error_function = tf.reduce_mean(tf.nn.sigmoid(output_layer))\n",
    "training = tf.train.GradientDescentOptimizer(LEARNING_RATE).minimize(error_function)\n",
    "prediction=tf.sigmoid(output_layer)\n",
    "Human_Conc_TrainingTarget=Human_Conc_TrainingTarget.reshape(len(Human_Conc_TrainingTarget),1)\n",
    "with tf.Session() as sess:\n",
    "     tf.global_variables_initializer().run()\n",
    "     for epoch in range(0,1000):\n",
    "         sess.run(training, feed_dict={inputTensor: np.transpose(Human_Conc_TrainingData), \n",
    "                                          outputTensor: Human_Conc_TrainingTarget})\n",
    "     expectedoutput = sess.run(prediction, feed_dict={inputTensor: np.transpose(Human_Conc_TrainingData)})\n",
    "     TrainingAccuracy=calaccuracy(expectedoutput,Human_Conc_TrainingTarget)\n",
    "     expectedoutput = sess.run(prediction, feed_dict={inputTensor: np.transpose(Human_Conc_ValidationData)})\n",
    "     ValidationAccuracy=calaccuracy(expectedoutput,Human_Conc_ValidationTarget)\n",
    "     expectedoutput = sess.run(prediction, feed_dict={inputTensor: np.transpose(Human_Conc_TestingData)})\n",
    "     TestingAccuracy=calaccuracy(expectedoutput,Human_Conc_TestingTarget)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------Neural Network on Human Concatenation dataset--------------------\n",
      "Training Accuracy  = 48.21717990275527\n",
      "Validation Accuracy  = 53.246753246753244\n",
      "Testing Accuracy  = 49.01960784313725\n"
     ]
    }
   ],
   "source": [
    "print ('------------------Neural Network on Human Concatenation dataset--------------------')\n",
    "#print(str(L_Erms_TR))\n",
    "print (\"Training Accuracy  = \" + str(TrainingAccuracy))\n",
    "print (\"Validation Accuracy  = \" + str(ValidationAccuracy))\n",
    "print (\"Testing Accuracy  = \" + str(TestingAccuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Human Subtraction Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputlayer=9\n",
    "hiddenlayer=10\n",
    "outputlayer=1\n",
    "LEARNING_RATE = 0.05\n",
    "inputTensor=tf.placeholder(tf.float32,[None,inputlayer])\n",
    "outputTensor=tf.placeholder(tf.float32,[None,outputlayer])\n",
    "def init_weights(shape):\n",
    "        return tf.Variable(tf.random_normal(shape,stddev=0.01))\n",
    "input_hidden_weights  = init_weights([inputlayer, hiddenlayer])\n",
    "hidden_output_weights = init_weights([hiddenlayer, outputlayer])\n",
    "hidden_layer = tf.nn.sigmoid(tf.matmul(inputTensor, input_hidden_weights))\n",
    "output_layer = tf.matmul(hidden_layer, hidden_output_weights)\n",
    "error_function = tf.reduce_mean(tf.nn.sigmoid(output_layer))\n",
    "training = tf.train.GradientDescentOptimizer(LEARNING_RATE).minimize(error_function)\n",
    "prediction=tf.sigmoid(output_layer)\n",
    "Human_Subt_TrainingTarget=Human_Subt_TrainingTarget.reshape(len(Human_Subt_TrainingTarget),1)\n",
    "with tf.Session() as sess:\n",
    "     tf.global_variables_initializer().run()\n",
    "     for epoch in range(0,100):\n",
    "         sess.run(training, feed_dict={inputTensor: np.transpose(Human_Subt_TrainingData), \n",
    "                                          outputTensor: Human_Subt_TrainingTarget})\n",
    "     expectedoutput = sess.run(prediction, feed_dict={inputTensor: np.transpose(Human_Subt_TrainingData)})\n",
    "     TrainingAccuracy=calaccuracy(expectedoutput,Human_Subt_TrainingTarget)\n",
    "     expectedoutput = sess.run(prediction, feed_dict={inputTensor: np.transpose(Human_Subt_ValidationData)})\n",
    "     ValidationAccuracy=calaccuracy(expectedoutput,Human_Subt_ValidationTarget)\n",
    "     expectedoutput = sess.run(prediction, feed_dict={inputTensor: np.transpose(Human_Subt_TestingData)})\n",
    "     TestingAccuracy=calaccuracy(expectedoutput,Human_Subt_TestingTarget)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------Neural Network on Human Subtraction dataset--------------------\n",
      "Training Accuracy  = 48.21717990275527\n",
      "Validation Accuracy  = 53.246753246753244\n",
      "Testing Accuracy  = 49.01960784313725\n"
     ]
    }
   ],
   "source": [
    "print ('------------------Neural Network on Human Subtraction dataset--------------------')\n",
    "#print(str(L_Erms_TR))\n",
    "print (\"Training Accuracy  = \" + str(TrainingAccuracy))\n",
    "print (\"Validation Accuracy  = \" + str(ValidationAccuracy))\n",
    "print (\"Testing Accuracy  = \" + str(TestingAccuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network on GSC Concatenation Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(16890, 1)\n"
     ]
    }
   ],
   "source": [
    "inputlayer=1024\n",
    "hiddenlayer=10\n",
    "outputlayer=1\n",
    "LEARNING_RATE = 0.05\n",
    "inputTensor=tf.placeholder(tf.float32,[None,inputlayer])\n",
    "outputTensor=tf.placeholder(tf.float32,[None,outputlayer])\n",
    "def init_weights(shape):\n",
    "        return tf.Variable(tf.random_normal(shape,stddev=0.01))\n",
    "input_hidden_weights  = init_weights([inputlayer, hiddenlayer])\n",
    "hidden_output_weights = init_weights([hiddenlayer, outputlayer])\n",
    "hidden_layer = tf.nn.sigmoid(tf.matmul(inputTensor, input_hidden_weights))\n",
    "output_layer = tf.matmul(hidden_layer, hidden_output_weights)\n",
    "error_function = tf.reduce_mean(tf.nn.sigmoid(output_layer))\n",
    "training = tf.train.GradientDescentOptimizer(LEARNING_RATE).minimize(error_function)\n",
    "prediction=tf.sigmoid(output_layer)\n",
    "Gsc_Conc_TrainingTarget=Gsc_Conc_TrainingTarget.reshape(len(Gsc_Conc_TrainingTarget),1)\n",
    "with tf.Session() as sess:\n",
    "     tf.global_variables_initializer().run()\n",
    "     for epoch in range(0,1000):\n",
    "         sess.run(training, feed_dict={inputTensor: np.transpose(Gsc_Conc_TrainingData), \n",
    "                                          outputTensor: Gsc_Conc_TrainingTarget})\n",
    "     expectedoutput = sess.run(prediction, feed_dict={inputTensor: np.transpose(Gsc_Conc_TrainingData)})\n",
    "     TrainingAccuracy=calaccuracy(expectedoutput,Gsc_Conc_TrainingTarget)\n",
    "     expectedoutput = sess.run(prediction, feed_dict={inputTensor: np.transpose(Gsc_Conc_ValidationData)})\n",
    "     ValidationAccuracy=calaccuracy(expectedoutput,Gsc_Conc_ValidationTarget)\n",
    "     expectedoutput = sess.run(prediction, feed_dict={inputTensor: np.transpose(Gsc_Conc_TestingData)})\n",
    "     TestingAccuracy=calaccuracy(expectedoutput,Gsc_Conc_TestingTarget)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------Neural Network on GSC Concatenation dataset--------------------\n",
      "Training Accuracy  = 51.70515097690941\n",
      "Validation Accuracy  = 50.639507342491704\n",
      "Testing Accuracy  = 51.75355450236967\n"
     ]
    }
   ],
   "source": [
    "print ('------------------Neural Network on GSC Concatenation dataset--------------------')\n",
    "#print(str(L_Erms_TR))\n",
    "print (\"Training Accuracy  = \" + str(TrainingAccuracy))\n",
    "print (\"Validation Accuracy  = \" + str(ValidationAccuracy))\n",
    "print (\"Testing Accuracy  = \" + str(TestingAccuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network On Gsc Subtraction Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputlayer=512\n",
    "hiddenlayer=10\n",
    "outputlayer=1\n",
    "LEARNING_RATE = 0.05\n",
    "inputTensor=tf.placeholder(tf.float32,[None,inputlayer])\n",
    "outputTensor=tf.placeholder(tf.float32,[None,outputlayer])\n",
    "def init_weights(shape):\n",
    "        return tf.Variable(tf.random_normal(shape,stddev=0.01))\n",
    "input_hidden_weights  = init_weights([inputlayer, hiddenlayer])\n",
    "hidden_output_weights = init_weights([hiddenlayer, outputlayer])\n",
    "hidden_layer = tf.nn.sigmoid(tf.matmul(inputTensor, input_hidden_weights))\n",
    "output_layer = tf.matmul(hidden_layer, hidden_output_weights)\n",
    "error_function = tf.reduce_mean(tf.nn.sigmoid(output_layer))\n",
    "training = tf.train.GradientDescentOptimizer(LEARNING_RATE).minimize(error_function)\n",
    "prediction=tf.sigmoid(output_layer)\n",
    "Gsc_Subt_TrainingTarget=Gsc_Conc_TrainingTarget.reshape(len(Gsc_Subt_TrainingTarget),1)\n",
    "with tf.Session() as sess:\n",
    "     tf.global_variables_initializer().run()\n",
    "     for epoch in range(0,1000):\n",
    "         # Training The network\n",
    "         sess.run(training, feed_dict={inputTensor: np.transpose(Gsc_Subt_TrainingData), \n",
    "                                          outputTensor: Gsc_Subt_TrainingTarget})\n",
    "     # Calculating Training Accuracy\n",
    "     expectedoutput = sess.run(prediction, feed_dict={inputTensor: np.transpose(Gsc_Subt_TrainingData)})\n",
    "     TrainingAccuracy=calaccuracy(expectedoutput,Gsc_Subt_TrainingTarget)\n",
    "    # Calculating Validation Accuracy\n",
    "     expectedoutput = sess.run(prediction, feed_dict={inputTensor: np.transpose(Gsc_Subt_ValidationData)})\n",
    "     ValidationAccuracy=calaccuracy(expectedoutput,Gsc_Subt_ValidationTarget)\n",
    "    # Caluclating Testing Accuracy\n",
    "     expectedoutput = sess.run(prediction, feed_dict={inputTensor: np.transpose(Gsc_Subt_TestingData)})\n",
    "     TestingAccuracy=calaccuracy(expectedoutput,Gsc_Subt_TestingTarget)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------Neural Network on GSC Subtraction dataset--------------------\n",
      "Training Accuracy  = 51.70515097690941\n",
      "Validation Accuracy  = 50.87636191378494\n",
      "Testing Accuracy  = 52.51184834123222\n"
     ]
    }
   ],
   "source": [
    "print ('------------------Neural Network on GSC Subtraction dataset--------------------')\n",
    "#print(str(L_Erms_TR))\n",
    "print (\"Training Accuracy  = \" + str(TrainingAccuracy))\n",
    "print (\"Validation Accuracy  = \" + str(ValidationAccuracy))\n",
    "print (\"Testing Accuracy  = \" + str(TestingAccuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
